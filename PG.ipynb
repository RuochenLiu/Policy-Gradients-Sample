{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "MODEL_LOG_DIR = '../log/'\n",
    "TRAIN_LOG_DIR = MODEL_LOG_DIR + 'train/' + 'pg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(y, n_class):\n",
    "    y = np.array(y)\n",
    "    res = np.eye(n_class)[np.array(y).reshape(-1)]\n",
    "    return res.reshape(list(y.shape)+[n_class])\n",
    "\n",
    "def get_discounted_values(values, gamma):\n",
    "    n = len(values)\n",
    "    discounted_values = np.zeros_like(values)\n",
    "    accumulative = 0.0\n",
    "    index = list(range(n))\n",
    "    index.reverse()\n",
    "    for i in index:\n",
    "        accumulative = values[i] + gamma * accumulative\n",
    "        discounted_values[i] = accumulative\n",
    "        \n",
    "    discounted_values = (discounted_values - np.mean(discounted_values))/np.std(discounted_values)\n",
    "    return discounted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, session, n_in, n_out):\n",
    "        \n",
    "        self.session = session\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.global_step = 0\n",
    "        \n",
    "        self.n1 = 10\n",
    "        self.n2 = 2\n",
    "        \n",
    "        with tf.name_scope('PlaceHolders'):\n",
    "            self.states = tf.placeholder(tf.float32, [None, self.n_in], name='states')\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.n_out], name='actions')\n",
    "            self.discounted_rewards = tf.placeholder(tf.float32, [None, ], name='dist_rewards')\n",
    "            \n",
    "            self.reward = tf.placeholder(tf.float32, [], name='reward')\n",
    "            self.rewards_mean = tf.placeholder(tf.float32, [], name='rewards')\n",
    "            \n",
    "        with tf.name_scope('FullyConnectedLayer_1'):\n",
    "            self.W_fc1 = tf.get_variable('W_fc1', shape=[self.n_in, self.n1])\n",
    "            self.b_fc1 = tf.get_variable('b_fc1', shape=[self.n1])\n",
    "            self.h_fc1 = tf.nn.relu(tf.add(tf.matmul(self.states, self.W_fc1), self.b_fc1))\n",
    "            \n",
    "        with tf.name_scope('FullyConnectedLayer_2'):\n",
    "            self.W_fc2 = tf.get_variable('W_fc2', shape=[self.n1,self.n2])\n",
    "            self.b_fc2 = tf.get_variable('b_fc2', shape=[self.n2])\n",
    "            self.h_fc2 = tf.nn.relu(tf.add(tf.matmul(self.h_fc1, self.W_fc2), self.b_fc2))\n",
    "            \n",
    "        with tf.name_scope('PolicyLayer'):\n",
    "            self.W_fc3 = tf.get_variable('W_fc5', shape=[self.n2, self.n_out])\n",
    "            self.b_fc3 = tf.get_variable('b_fc5', shape=[self.n_out])\n",
    "            self.action_proba = tf.nn.softmax(tf.add(tf.matmul(self.h_fc2, self.W_fc3), self.b_fc3), name='Policy')\n",
    "        \n",
    "        with tf.name_scope('LearningRate'):\n",
    "            self.lr = 0.01\n",
    "            \n",
    "        with tf.name_scope('Loss'):\n",
    "            self.neg_log_proba = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.action_proba, labels = self.actions)\n",
    "            self.loss = tf.reduce_mean(self.discounted_rewards * self.neg_log_proba)\n",
    "            \n",
    "        with tf.name_scope('TrainStep'):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        \n",
    "        with tf.name_scope('SummaryWriter'):\n",
    "            self.train_summary = tf.summary.merge([tf.summary.scalar(\"train_loss\", self.loss)])\n",
    "            self.reward_summary = tf.summary.merge([tf.summary.scalar(\"episode_reward\", self.reward)])\n",
    "            self.mean_reward_summary = tf.summary.merge([tf.summary.scalar(\"mean_episode_reward_50\", self.rewards_mean)])\n",
    "            self.writer = tf.summary.FileWriter(TRAIN_LOG_DIR, session.graph)\n",
    "            \n",
    "        self.init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.session.run(self.init)\n",
    "\n",
    "    def compute_action_proba(self, x):\n",
    "        return self.session.run(self.action_proba, feed_dict={self.states:np.reshape(x,[-1, self.n_in])})\n",
    "    \n",
    "    def train(self, ep_states, ep_actions, ep_rewards):\n",
    "        self.global_step += 1\n",
    "        self.summary, _ = self.session.run([self.train_summary, self.train_step], feed_dict={self.states: ep_states, self.actions: ep_actions, self.discounted_rewards: ep_rewards})\n",
    "        self.writer.add_summary(self.summary, self.global_step)\n",
    "        \n",
    "    def write_reward(self, ep, r, r_mean):\n",
    "        self.summary_0, self.summary_1 = self.session.run([self.reward_summary, self.mean_reward_summary], feed_dict={self.reward: r, self.rewards_mean: r_mean})\n",
    "        self.writer.add_summary(self.summary_0, ep)\n",
    "        self.writer.add_summary(self.summary_1, ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, tf_session, env):\n",
    "        self.env = env\n",
    "        self.obs_space = env.observation_space.shape[0]\n",
    "        try:\n",
    "            self.act_space = env.action_space.n\n",
    "        except:\n",
    "            self.act_space = env.action_space.shape[0]\n",
    "        self.gamma = 0.95\n",
    "        self.policy = Network(tf_session, self.obs_space, self.act_space)\n",
    "        \n",
    "        self.ep_states = []\n",
    "        self.ep_actions = []\n",
    "        self.ep_rewards = []\n",
    "\n",
    "    def gather_exp(self, observation, action, reward):\n",
    "        self.ep_states.append(observation)\n",
    "        self.ep_actions.append(action)\n",
    "        self.ep_rewards.append(reward)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        action_prob = self.policy.compute_action_proba(observation).ravel()\n",
    "        return np.random.choice(list(range(self.act_space)),p = action_prob)\n",
    "    \n",
    "    def choose_random_action(self):\n",
    "        return np.random.choice(list(range(self.act_space)))\n",
    "\n",
    "    def update_policy(self):\n",
    "        self.one_hot_actions = get_one_hot(self.ep_actions, self.act_space)\n",
    "        self.discounted_rewards = get_discounted_values(self.ep_rewards, self.gamma)\n",
    "        self.policy.train(self.ep_states, self.one_hot_actions,self.discounted_rewards)\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def gather_reward(self, reward):\n",
    "        self.ep_rewards.append(reward)\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return sum(self.ep_rewards)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.ep_states = []\n",
    "        self.ep_actions = []\n",
    "        self.ep_rewards = []\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ruochenliu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Training starts -- 20:50:24\n",
      "==========================\n",
      "\n",
      "Episode 1 with reward 44.0\n",
      "\n",
      "Mean reward 44.0\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 2 with reward 10.0\n",
      "\n",
      "Mean reward 27.0\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 3 with reward 14.0\n",
      "\n",
      "Mean reward 22.666666666666668\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 4 with reward 17.0\n",
      "\n",
      "Mean reward 21.25\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 5 with reward 17.0\n",
      "\n",
      "Mean reward 20.4\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 6 with reward 25.0\n",
      "\n",
      "Mean reward 21.166666666666668\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 7 with reward 40.0\n",
      "\n",
      "Mean reward 23.857142857142858\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 8 with reward 23.0\n",
      "\n",
      "Mean reward 23.75\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 9 with reward 14.0\n",
      "\n",
      "Mean reward 22.666666666666668\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 10 with reward 32.0\n",
      "\n",
      "Mean reward 23.6\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 11 with reward 19.0\n",
      "\n",
      "Mean reward 23.181818181818183\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 12 with reward 13.0\n",
      "\n",
      "Mean reward 22.333333333333332\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 13 with reward 11.0\n",
      "\n",
      "Mean reward 21.46153846153846\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 14 with reward 19.0\n",
      "\n",
      "Mean reward 21.285714285714285\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 15 with reward 13.0\n",
      "\n",
      "Mean reward 20.733333333333334\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 16 with reward 17.0\n",
      "\n",
      "Mean reward 20.5\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 17 with reward 15.0\n",
      "\n",
      "Mean reward 20.176470588235293\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 18 with reward 18.0\n",
      "\n",
      "Mean reward 20.055555555555557\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 19 with reward 10.0\n",
      "\n",
      "Mean reward 19.526315789473685\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 20 with reward 24.0\n",
      "\n",
      "Mean reward 19.75\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 21 with reward 23.0\n",
      "\n",
      "Mean reward 19.904761904761905\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 22 with reward 12.0\n",
      "\n",
      "Mean reward 19.545454545454547\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 23 with reward 31.0\n",
      "\n",
      "Mean reward 20.043478260869566\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 24 with reward 13.0\n",
      "\n",
      "Mean reward 19.75\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 25 with reward 20.0\n",
      "\n",
      "Mean reward 19.76\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 26 with reward 11.0\n",
      "\n",
      "Mean reward 19.423076923076923\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 27 with reward 13.0\n",
      "\n",
      "Mean reward 19.185185185185187\n",
      "\n",
      "Max reward 44.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 28 with reward 50.0\n",
      "\n",
      "Mean reward 20.285714285714285\n",
      "\n",
      "Max reward 50.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 29 with reward 17.0\n",
      "\n",
      "Mean reward 20.17241379310345\n",
      "\n",
      "Max reward 50.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 30 with reward 10.0\n",
      "\n",
      "Mean reward 19.833333333333332\n",
      "\n",
      "Max reward 50.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 31 with reward 13.0\n",
      "\n",
      "Mean reward 19.612903225806452\n",
      "\n",
      "Max reward 50.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 32 with reward 10.0\n",
      "\n",
      "Mean reward 19.3125\n",
      "\n",
      "Max reward 50.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 33 with reward 13.0\n",
      "\n",
      "Mean reward 19.12121212121212\n",
      "\n",
      "Max reward 50.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 34 with reward 54.0\n",
      "\n",
      "Mean reward 20.147058823529413\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 35 with reward 12.0\n",
      "\n",
      "Mean reward 19.914285714285715\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 36 with reward 37.0\n",
      "\n",
      "Mean reward 20.38888888888889\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 37 with reward 22.0\n",
      "\n",
      "Mean reward 20.43243243243243\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 38 with reward 14.0\n",
      "\n",
      "Mean reward 20.263157894736842\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 39 with reward 18.0\n",
      "\n",
      "Mean reward 20.205128205128204\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 40 with reward 27.0\n",
      "\n",
      "Mean reward 20.375\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 41 with reward 21.0\n",
      "\n",
      "Mean reward 20.390243902439025\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 42 with reward 8.0\n",
      "\n",
      "Mean reward 20.095238095238095\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 43 with reward 14.0\n",
      "\n",
      "Mean reward 19.953488372093023\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 44 with reward 11.0\n",
      "\n",
      "Mean reward 19.75\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 45 with reward 14.0\n",
      "\n",
      "Mean reward 19.622222222222224\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 46 with reward 35.0\n",
      "\n",
      "Mean reward 19.956521739130434\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 47 with reward 11.0\n",
      "\n",
      "Mean reward 19.76595744680851\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 48 with reward 24.0\n",
      "\n",
      "Mean reward 19.854166666666668\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 49 with reward 14.0\n",
      "\n",
      "Mean reward 19.73469387755102\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 50 with reward 17.0\n",
      "\n",
      "Mean reward 19.68\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 51 with reward 23.0\n",
      "\n",
      "Mean reward 19.745098039215687\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 52 with reward 18.0\n",
      "\n",
      "Mean reward 19.71153846153846\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 53 with reward 13.0\n",
      "\n",
      "Mean reward 19.58490566037736\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 54 with reward 13.0\n",
      "\n",
      "Mean reward 19.462962962962962\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 55 with reward 10.0\n",
      "\n",
      "Mean reward 19.29090909090909\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 56 with reward 44.0\n",
      "\n",
      "Mean reward 19.732142857142858\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 57 with reward 14.0\n",
      "\n",
      "Mean reward 19.63157894736842\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 58 with reward 9.0\n",
      "\n",
      "Mean reward 19.448275862068964\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 59 with reward 26.0\n",
      "\n",
      "Mean reward 19.559322033898304\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 60 with reward 8.0\n",
      "\n",
      "Mean reward 19.366666666666667\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 61 with reward 12.0\n",
      "\n",
      "Mean reward 19.24590163934426\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 62 with reward 29.0\n",
      "\n",
      "Mean reward 19.403225806451612\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 63 with reward 10.0\n",
      "\n",
      "Mean reward 19.253968253968253\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 64 with reward 15.0\n",
      "\n",
      "Mean reward 19.1875\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 65 with reward 16.0\n",
      "\n",
      "Mean reward 19.138461538461538\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 66 with reward 12.0\n",
      "\n",
      "Mean reward 19.03030303030303\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 67 with reward 15.0\n",
      "\n",
      "Mean reward 18.970149253731343\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 68 with reward 27.0\n",
      "\n",
      "Mean reward 19.08823529411765\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 69 with reward 14.0\n",
      "\n",
      "Mean reward 19.014492753623188\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 70 with reward 9.0\n",
      "\n",
      "Mean reward 18.87142857142857\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 71 with reward 34.0\n",
      "\n",
      "Mean reward 19.08450704225352\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 72 with reward 11.0\n",
      "\n",
      "Mean reward 18.97222222222222\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 73 with reward 9.0\n",
      "\n",
      "Mean reward 18.835616438356166\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 74 with reward 17.0\n",
      "\n",
      "Mean reward 18.81081081081081\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 75 with reward 13.0\n",
      "\n",
      "Mean reward 18.733333333333334\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 76 with reward 31.0\n",
      "\n",
      "Mean reward 18.894736842105264\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 77 with reward 11.0\n",
      "\n",
      "Mean reward 18.792207792207794\n",
      "\n",
      "Max reward 54.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "\n",
      "Episode 78 with reward 43.0\n",
      "\n",
      "Mean reward 19.102564102564102\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 79 with reward 13.0\n",
      "\n",
      "Mean reward 19.025316455696203\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 80 with reward 12.0\n",
      "\n",
      "Mean reward 18.9375\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 81 with reward 19.0\n",
      "\n",
      "Mean reward 18.938271604938272\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 82 with reward 34.0\n",
      "\n",
      "Mean reward 19.121951219512194\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 83 with reward 15.0\n",
      "\n",
      "Mean reward 19.072289156626507\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 84 with reward 33.0\n",
      "\n",
      "Mean reward 19.238095238095237\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 85 with reward 14.0\n",
      "\n",
      "Mean reward 19.176470588235293\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 86 with reward 17.0\n",
      "\n",
      "Mean reward 19.151162790697676\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 87 with reward 16.0\n",
      "\n",
      "Mean reward 19.114942528735632\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 88 with reward 14.0\n",
      "\n",
      "Mean reward 19.056818181818183\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 89 with reward 13.0\n",
      "\n",
      "Mean reward 18.98876404494382\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 90 with reward 17.0\n",
      "\n",
      "Mean reward 18.966666666666665\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 91 with reward 53.0\n",
      "\n",
      "Mean reward 19.34065934065934\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 92 with reward 23.0\n",
      "\n",
      "Mean reward 19.380434782608695\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 93 with reward 11.0\n",
      "\n",
      "Mean reward 19.29032258064516\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 94 with reward 15.0\n",
      "\n",
      "Mean reward 19.24468085106383\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 95 with reward 17.0\n",
      "\n",
      "Mean reward 19.221052631578946\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 96 with reward 12.0\n",
      "\n",
      "Mean reward 19.145833333333332\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 97 with reward 11.0\n",
      "\n",
      "Mean reward 19.061855670103093\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 98 with reward 16.0\n",
      "\n",
      "Mean reward 19.03061224489796\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 99 with reward 11.0\n",
      "\n",
      "Mean reward 18.949494949494948\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 100 with reward 14.0\n",
      "\n",
      "Mean reward 18.9\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 101 with reward 33.0\n",
      "\n",
      "Mean reward 19.03960396039604\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 102 with reward 11.0\n",
      "\n",
      "Mean reward 18.96078431372549\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 103 with reward 15.0\n",
      "\n",
      "Mean reward 18.92233009708738\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 104 with reward 9.0\n",
      "\n",
      "Mean reward 18.826923076923077\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 105 with reward 13.0\n",
      "\n",
      "Mean reward 18.771428571428572\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 106 with reward 22.0\n",
      "\n",
      "Mean reward 18.80188679245283\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 107 with reward 16.0\n",
      "\n",
      "Mean reward 18.77570093457944\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 108 with reward 14.0\n",
      "\n",
      "Mean reward 18.73148148148148\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 109 with reward 10.0\n",
      "\n",
      "Mean reward 18.65137614678899\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 110 with reward 13.0\n",
      "\n",
      "Mean reward 18.6\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 111 with reward 15.0\n",
      "\n",
      "Mean reward 18.56756756756757\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 112 with reward 24.0\n",
      "\n",
      "Mean reward 18.616071428571427\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 113 with reward 13.0\n",
      "\n",
      "Mean reward 18.56637168141593\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 114 with reward 12.0\n",
      "\n",
      "Mean reward 18.50877192982456\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 115 with reward 22.0\n",
      "\n",
      "Mean reward 18.53913043478261\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 116 with reward 13.0\n",
      "\n",
      "Mean reward 18.49137931034483\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 117 with reward 24.0\n",
      "\n",
      "Mean reward 18.53846153846154\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 118 with reward 26.0\n",
      "\n",
      "Mean reward 18.60169491525424\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 119 with reward 14.0\n",
      "\n",
      "Mean reward 18.563025210084035\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 120 with reward 19.0\n",
      "\n",
      "Mean reward 18.566666666666666\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 121 with reward 14.0\n",
      "\n",
      "Mean reward 18.52892561983471\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 122 with reward 17.0\n",
      "\n",
      "Mean reward 18.516393442622952\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 123 with reward 11.0\n",
      "\n",
      "Mean reward 18.45528455284553\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 124 with reward 28.0\n",
      "\n",
      "Mean reward 18.532258064516128\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 125 with reward 13.0\n",
      "\n",
      "Mean reward 18.488\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 126 with reward 28.0\n",
      "\n",
      "Mean reward 18.563492063492063\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 127 with reward 14.0\n",
      "\n",
      "Mean reward 18.52755905511811\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 128 with reward 26.0\n",
      "\n",
      "Mean reward 18.5859375\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 129 with reward 8.0\n",
      "\n",
      "Mean reward 18.503875968992247\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 130 with reward 11.0\n",
      "\n",
      "Mean reward 18.446153846153845\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 131 with reward 11.0\n",
      "\n",
      "Mean reward 18.389312977099237\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 132 with reward 11.0\n",
      "\n",
      "Mean reward 18.333333333333332\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 133 with reward 20.0\n",
      "\n",
      "Mean reward 18.345864661654137\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 134 with reward 18.0\n",
      "\n",
      "Mean reward 18.34328358208955\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 135 with reward 15.0\n",
      "\n",
      "Mean reward 18.31851851851852\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 136 with reward 18.0\n",
      "\n",
      "Mean reward 18.316176470588236\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 137 with reward 12.0\n",
      "\n",
      "Mean reward 18.27007299270073\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 138 with reward 23.0\n",
      "\n",
      "Mean reward 18.304347826086957\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 139 with reward 17.0\n",
      "\n",
      "Mean reward 18.294964028776977\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 140 with reward 12.0\n",
      "\n",
      "Mean reward 18.25\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 141 with reward 16.0\n",
      "\n",
      "Mean reward 18.23404255319149\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 142 with reward 13.0\n",
      "\n",
      "Mean reward 18.197183098591548\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 143 with reward 22.0\n",
      "\n",
      "Mean reward 18.223776223776223\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 144 with reward 22.0\n",
      "\n",
      "Mean reward 18.25\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 145 with reward 39.0\n",
      "\n",
      "Mean reward 18.393103448275863\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 146 with reward 10.0\n",
      "\n",
      "Mean reward 18.335616438356166\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 147 with reward 22.0\n",
      "\n",
      "Mean reward 18.360544217687075\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 148 with reward 26.0\n",
      "\n",
      "Mean reward 18.41216216216216\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 149 with reward 16.0\n",
      "\n",
      "Mean reward 18.395973154362416\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 150 with reward 15.0\n",
      "\n",
      "Mean reward 18.373333333333335\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 151 with reward 13.0\n",
      "\n",
      "Mean reward 18.337748344370862\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 152 with reward 20.0\n",
      "\n",
      "Mean reward 18.348684210526315\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 153 with reward 29.0\n",
      "\n",
      "Mean reward 18.41830065359477\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 154 with reward 14.0\n",
      "\n",
      "Mean reward 18.38961038961039\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 155 with reward 10.0\n",
      "\n",
      "Mean reward 18.335483870967742\n",
      "\n",
      "Max reward 54.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "\n",
      "Episode 156 with reward 11.0\n",
      "\n",
      "Mean reward 18.28846153846154\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 157 with reward 16.0\n",
      "\n",
      "Mean reward 18.273885350318473\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 158 with reward 12.0\n",
      "\n",
      "Mean reward 18.234177215189874\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 159 with reward 41.0\n",
      "\n",
      "Mean reward 18.37735849056604\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 160 with reward 19.0\n",
      "\n",
      "Mean reward 18.38125\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 161 with reward 10.0\n",
      "\n",
      "Mean reward 18.32919254658385\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 162 with reward 28.0\n",
      "\n",
      "Mean reward 18.38888888888889\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 163 with reward 9.0\n",
      "\n",
      "Mean reward 18.33128834355828\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 164 with reward 10.0\n",
      "\n",
      "Mean reward 18.28048780487805\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 165 with reward 16.0\n",
      "\n",
      "Mean reward 18.266666666666666\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 166 with reward 14.0\n",
      "\n",
      "Mean reward 18.240963855421686\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 167 with reward 26.0\n",
      "\n",
      "Mean reward 18.2874251497006\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 168 with reward 10.0\n",
      "\n",
      "Mean reward 18.238095238095237\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 169 with reward 35.0\n",
      "\n",
      "Mean reward 18.337278106508876\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 170 with reward 34.0\n",
      "\n",
      "Mean reward 18.429411764705883\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 171 with reward 9.0\n",
      "\n",
      "Mean reward 18.374269005847953\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 172 with reward 14.0\n",
      "\n",
      "Mean reward 18.348837209302324\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 173 with reward 25.0\n",
      "\n",
      "Mean reward 18.387283236994218\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 174 with reward 23.0\n",
      "\n",
      "Mean reward 18.413793103448278\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 175 with reward 14.0\n",
      "\n",
      "Mean reward 18.388571428571428\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 176 with reward 11.0\n",
      "\n",
      "Mean reward 18.34659090909091\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 177 with reward 12.0\n",
      "\n",
      "Mean reward 18.310734463276837\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 178 with reward 19.0\n",
      "\n",
      "Mean reward 18.314606741573034\n",
      "\n",
      "Max reward 54.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 179 with reward 57.0\n",
      "\n",
      "Mean reward 18.53072625698324\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 180 with reward 17.0\n",
      "\n",
      "Mean reward 18.522222222222222\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 181 with reward 28.0\n",
      "\n",
      "Mean reward 18.574585635359117\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 182 with reward 12.0\n",
      "\n",
      "Mean reward 18.53846153846154\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 183 with reward 15.0\n",
      "\n",
      "Mean reward 18.51912568306011\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 184 with reward 10.0\n",
      "\n",
      "Mean reward 18.472826086956523\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 185 with reward 12.0\n",
      "\n",
      "Mean reward 18.437837837837836\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 186 with reward 13.0\n",
      "\n",
      "Mean reward 18.408602150537636\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 187 with reward 12.0\n",
      "\n",
      "Mean reward 18.37433155080214\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 188 with reward 16.0\n",
      "\n",
      "Mean reward 18.361702127659573\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 189 with reward 15.0\n",
      "\n",
      "Mean reward 18.343915343915345\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 190 with reward 9.0\n",
      "\n",
      "Mean reward 18.294736842105262\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 191 with reward 14.0\n",
      "\n",
      "Mean reward 18.272251308900522\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 192 with reward 11.0\n",
      "\n",
      "Mean reward 18.234375\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 193 with reward 14.0\n",
      "\n",
      "Mean reward 18.212435233160623\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 194 with reward 18.0\n",
      "\n",
      "Mean reward 18.211340206185568\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 195 with reward 12.0\n",
      "\n",
      "Mean reward 18.17948717948718\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 196 with reward 16.0\n",
      "\n",
      "Mean reward 18.168367346938776\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 197 with reward 14.0\n",
      "\n",
      "Mean reward 18.14720812182741\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 198 with reward 25.0\n",
      "\n",
      "Mean reward 18.181818181818183\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 199 with reward 54.0\n",
      "\n",
      "Mean reward 18.36180904522613\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 200 with reward 14.0\n",
      "\n",
      "Mean reward 18.34\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 201 with reward 14.0\n",
      "\n",
      "Mean reward 18.318407960199004\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 202 with reward 14.0\n",
      "\n",
      "Mean reward 18.297029702970296\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 203 with reward 25.0\n",
      "\n",
      "Mean reward 18.330049261083744\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 204 with reward 23.0\n",
      "\n",
      "Mean reward 18.352941176470587\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 205 with reward 33.0\n",
      "\n",
      "Mean reward 18.42439024390244\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 206 with reward 14.0\n",
      "\n",
      "Mean reward 18.402912621359224\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 207 with reward 11.0\n",
      "\n",
      "Mean reward 18.367149758454108\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 208 with reward 22.0\n",
      "\n",
      "Mean reward 18.384615384615383\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 209 with reward 10.0\n",
      "\n",
      "Mean reward 18.3444976076555\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 210 with reward 24.0\n",
      "\n",
      "Mean reward 18.37142857142857\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 211 with reward 11.0\n",
      "\n",
      "Mean reward 18.33649289099526\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 212 with reward 36.0\n",
      "\n",
      "Mean reward 18.419811320754718\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 213 with reward 14.0\n",
      "\n",
      "Mean reward 18.399061032863848\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 214 with reward 21.0\n",
      "\n",
      "Mean reward 18.411214953271028\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 215 with reward 21.0\n",
      "\n",
      "Mean reward 18.42325581395349\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 216 with reward 31.0\n",
      "\n",
      "Mean reward 18.48148148148148\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 217 with reward 11.0\n",
      "\n",
      "Mean reward 18.447004608294932\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 218 with reward 22.0\n",
      "\n",
      "Mean reward 18.46330275229358\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 219 with reward 13.0\n",
      "\n",
      "Mean reward 18.438356164383563\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 220 with reward 17.0\n",
      "\n",
      "Mean reward 18.431818181818183\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 221 with reward 23.0\n",
      "\n",
      "Mean reward 18.452488687782804\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 222 with reward 46.0\n",
      "\n",
      "Mean reward 18.576576576576578\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 223 with reward 23.0\n",
      "\n",
      "Mean reward 18.59641255605381\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 224 with reward 16.0\n",
      "\n",
      "Mean reward 18.584821428571427\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 225 with reward 14.0\n",
      "\n",
      "Mean reward 18.564444444444444\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 226 with reward 12.0\n",
      "\n",
      "Mean reward 18.535398230088497\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 227 with reward 23.0\n",
      "\n",
      "Mean reward 18.555066079295155\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 228 with reward 12.0\n",
      "\n",
      "Mean reward 18.526315789473685\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 229 with reward 31.0\n",
      "\n",
      "Mean reward 18.580786026200872\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 230 with reward 23.0\n",
      "\n",
      "Mean reward 18.6\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 231 with reward 11.0\n",
      "\n",
      "Mean reward 18.567099567099568\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 232 with reward 12.0\n",
      "\n",
      "Mean reward 18.538793103448278\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 233 with reward 12.0\n",
      "\n",
      "Mean reward 18.510729613733904\n",
      "\n",
      "Max reward 57.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "\n",
      "Episode 234 with reward 15.0\n",
      "\n",
      "Mean reward 18.495726495726494\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 235 with reward 23.0\n",
      "\n",
      "Mean reward 18.514893617021276\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 236 with reward 19.0\n",
      "\n",
      "Mean reward 18.516949152542374\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 237 with reward 11.0\n",
      "\n",
      "Mean reward 18.485232067510548\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 238 with reward 19.0\n",
      "\n",
      "Mean reward 18.48739495798319\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 239 with reward 15.0\n",
      "\n",
      "Mean reward 18.472803347280333\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 240 with reward 9.0\n",
      "\n",
      "Mean reward 18.433333333333334\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 241 with reward 17.0\n",
      "\n",
      "Mean reward 18.42738589211618\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 242 with reward 11.0\n",
      "\n",
      "Mean reward 18.39669421487603\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 243 with reward 14.0\n",
      "\n",
      "Mean reward 18.378600823045268\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 244 with reward 20.0\n",
      "\n",
      "Mean reward 18.385245901639344\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 245 with reward 12.0\n",
      "\n",
      "Mean reward 18.35918367346939\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 246 with reward 24.0\n",
      "\n",
      "Mean reward 18.382113821138212\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 247 with reward 22.0\n",
      "\n",
      "Mean reward 18.39676113360324\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 248 with reward 11.0\n",
      "\n",
      "Mean reward 18.366935483870968\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 249 with reward 11.0\n",
      "\n",
      "Mean reward 18.337349397590362\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 250 with reward 14.0\n",
      "\n",
      "Mean reward 18.32\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 251 with reward 14.0\n",
      "\n",
      "Mean reward 18.302788844621514\n",
      "\n",
      "Max reward 57.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 252 with reward 65.0\n",
      "\n",
      "Mean reward 18.488095238095237\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 253 with reward 12.0\n",
      "\n",
      "Mean reward 18.462450592885375\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 254 with reward 11.0\n",
      "\n",
      "Mean reward 18.433070866141733\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 255 with reward 23.0\n",
      "\n",
      "Mean reward 18.45098039215686\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 256 with reward 14.0\n",
      "\n",
      "Mean reward 18.43359375\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 257 with reward 18.0\n",
      "\n",
      "Mean reward 18.43190661478599\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 258 with reward 13.0\n",
      "\n",
      "Mean reward 18.410852713178294\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 259 with reward 20.0\n",
      "\n",
      "Mean reward 18.416988416988417\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 260 with reward 12.0\n",
      "\n",
      "Mean reward 18.392307692307693\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 261 with reward 20.0\n",
      "\n",
      "Mean reward 18.39846743295019\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 262 with reward 11.0\n",
      "\n",
      "Mean reward 18.370229007633586\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 263 with reward 11.0\n",
      "\n",
      "Mean reward 18.342205323193916\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 264 with reward 15.0\n",
      "\n",
      "Mean reward 18.329545454545453\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 265 with reward 13.0\n",
      "\n",
      "Mean reward 18.30943396226415\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 266 with reward 32.0\n",
      "\n",
      "Mean reward 18.360902255639097\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 267 with reward 12.0\n",
      "\n",
      "Mean reward 18.337078651685392\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 268 with reward 10.0\n",
      "\n",
      "Mean reward 18.30597014925373\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 269 with reward 12.0\n",
      "\n",
      "Mean reward 18.28252788104089\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 270 with reward 12.0\n",
      "\n",
      "Mean reward 18.25925925925926\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 271 with reward 16.0\n",
      "\n",
      "Mean reward 18.25092250922509\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 272 with reward 27.0\n",
      "\n",
      "Mean reward 18.283088235294116\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 273 with reward 11.0\n",
      "\n",
      "Mean reward 18.256410256410255\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 274 with reward 11.0\n",
      "\n",
      "Mean reward 18.22992700729927\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 275 with reward 17.0\n",
      "\n",
      "Mean reward 18.225454545454546\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 276 with reward 19.0\n",
      "\n",
      "Mean reward 18.22826086956522\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 277 with reward 12.0\n",
      "\n",
      "Mean reward 18.205776173285198\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 278 with reward 12.0\n",
      "\n",
      "Mean reward 18.18345323741007\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 279 with reward 11.0\n",
      "\n",
      "Mean reward 18.157706093189965\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 280 with reward 12.0\n",
      "\n",
      "Mean reward 18.135714285714286\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 281 with reward 11.0\n",
      "\n",
      "Mean reward 18.110320284697508\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 282 with reward 11.0\n",
      "\n",
      "Mean reward 18.085106382978722\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 283 with reward 13.0\n",
      "\n",
      "Mean reward 18.06713780918728\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 284 with reward 19.0\n",
      "\n",
      "Mean reward 18.070422535211268\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 285 with reward 9.0\n",
      "\n",
      "Mean reward 18.03859649122807\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 286 with reward 23.0\n",
      "\n",
      "Mean reward 18.055944055944057\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 287 with reward 15.0\n",
      "\n",
      "Mean reward 18.045296167247386\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 288 with reward 18.0\n",
      "\n",
      "Mean reward 18.04513888888889\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 289 with reward 11.0\n",
      "\n",
      "Mean reward 18.02076124567474\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 290 with reward 31.0\n",
      "\n",
      "Mean reward 18.06551724137931\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 291 with reward 14.0\n",
      "\n",
      "Mean reward 18.051546391752577\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 292 with reward 24.0\n",
      "\n",
      "Mean reward 18.07191780821918\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 293 with reward 9.0\n",
      "\n",
      "Mean reward 18.040955631399317\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 294 with reward 18.0\n",
      "\n",
      "Mean reward 18.040816326530614\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 295 with reward 17.0\n",
      "\n",
      "Mean reward 18.03728813559322\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 296 with reward 19.0\n",
      "\n",
      "Mean reward 18.04054054054054\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 297 with reward 16.0\n",
      "\n",
      "Mean reward 18.033670033670035\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 298 with reward 20.0\n",
      "\n",
      "Mean reward 18.040268456375838\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 299 with reward 12.0\n",
      "\n",
      "Mean reward 18.02006688963211\n",
      "\n",
      "Max reward 65.0\n",
      "\n",
      "==========================\n",
      "\n",
      "Episode 300 with reward 15.0\n",
      "\n",
      "Mean reward 18.01\n",
      "\n",
      "Max reward 65.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../log/train/pg/-300'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_LOG_DIR):\n",
    "    os.makedirs(MODEL_LOG_DIR)\n",
    "if not os.path.exists(TRAIN_LOG_DIR):\n",
    "    os.makedirs(TRAIN_LOG_DIR)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "sess = tf.Session()\n",
    "player = Player(sess, env)  \n",
    "saver = tf.train.Saver()\n",
    "ep_rewards = []\n",
    "\n",
    "date_object = datetime.now()\n",
    "current_time = date_object.strftime('%H:%M:%S')\n",
    "print(\"Training starts -- {}\".format(current_time))\n",
    "\n",
    "for ep in range(300):\n",
    "    \n",
    "    observation = player.reset()\n",
    "    \n",
    "    while(True):\n",
    "        player.render()\n",
    "        action = player.choose_action(observation)\n",
    "        new_observation, reward, done, _ = player.take_action(action)\n",
    "        player.gather_exp(observation, action, reward)\n",
    "        observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            player.update_policy()\n",
    "            ep_rewards.append(player.get_total_reward())\n",
    "            player.policy.write_reward(ep+1, ep_rewards[-1], np.mean(ep_rewards[-50:]))\n",
    "            print('==========================\\n')\n",
    "            print('Episode {} with reward {}\\n'.format(ep+1, ep_rewards[-1]))\n",
    "            print('Mean reward {}\\n'.format(np.mean(ep_rewards)))\n",
    "            print('Max reward {}\\n'.format(np.max(ep_rewards)))\n",
    "            break\n",
    "\n",
    "saver.save(sess, TRAIN_LOG_DIR, global_step=ep+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.ep_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(np.array(player.one_hot_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(player.ep_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6459801 , 0.35401988], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_prob = player.policy.compute_action_proba(observation).ravel()\n",
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_one_hot([1,2,3,1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.obs_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
