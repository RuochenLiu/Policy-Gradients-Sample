{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "MODEL_LOG_DIR = '../log/'\n",
    "TRAIN_LOG_DIR = MODEL_LOG_DIR + 'train/' + 'pg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(y, n_class):\n",
    "    y = np.array(y)\n",
    "    res = np.eye(n_class)[np.array(y).reshape(-1)]\n",
    "    return res.reshape(list(y.shape)+[n_class])\n",
    "\n",
    "def get_discounted_values(values, gamma):\n",
    "    n = len(values)\n",
    "    discounted_values = np.zeros_like(values)\n",
    "    accumulative = 0.0\n",
    "    index = list(range(n))\n",
    "    index.reverse()\n",
    "    for i in index:\n",
    "        accumulative = values[i] + gamma * accumulative\n",
    "        discounted_values[i] = accumulative\n",
    "        \n",
    "    discounted_values = (discounted_values - np.mean(discounted_values))/np.std(discounted_values)\n",
    "    return discounted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, session, n_in, n_out):\n",
    "        \n",
    "        self.session = session\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.global_step = 0\n",
    "        \n",
    "        self.n1 = 10\n",
    "        self.n2 = 2\n",
    "        \n",
    "        with tf.name_scope('PlaceHolders'):\n",
    "            self.states = tf.placeholder(tf.float32, [None, self.n_in], name='states')\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.n_out], name='actions')\n",
    "            self.discounted_rewards = tf.placeholder(tf.float32, [None, ], name='dist_rewards')\n",
    "            \n",
    "            self.reward = tf.placeholder(tf.float32, [], name='reward')\n",
    "            self.rewards_mean = tf.placeholder(tf.float32, [], name='rewards')\n",
    "            \n",
    "        with tf.name_scope('FullyConnectedLayer_1'):\n",
    "            self.W_fc1 = tf.get_variable('W_fc1', shape=[self.n_in, self.n1])\n",
    "            self.b_fc1 = tf.get_variable('b_fc1', shape=[self.n1])\n",
    "            self.h_fc1 = tf.nn.relu(tf.add(tf.matmul(self.states, self.W_fc1), self.b_fc1))\n",
    "            \n",
    "        with tf.name_scope('FullyConnectedLayer_2'):\n",
    "            self.W_fc2 = tf.get_variable('W_fc2', shape=[self.n1,self.n2])\n",
    "            self.b_fc2 = tf.get_variable('b_fc2', shape=[self.n2])\n",
    "            self.h_fc2 = tf.nn.relu(tf.add(tf.matmul(self.h_fc1, self.W_fc2), self.b_fc2))\n",
    "            \n",
    "        with tf.name_scope('PolicyLayer'):\n",
    "            self.W_fc3 = tf.get_variable('W_fc5', shape=[self.n2, self.n_out])\n",
    "            self.b_fc3 = tf.get_variable('b_fc5', shape=[self.n_out])\n",
    "            self.action_proba = tf.nn.softmax(tf.add(tf.matmul(self.h_fc2, self.W_fc3), self.b_fc3), name='Policy')\n",
    "        \n",
    "        with tf.name_scope('LearningRate'):\n",
    "            self.lr = 0.01\n",
    "            \n",
    "        with tf.name_scope('Loss'):\n",
    "            self.neg_log_proba = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.action_proba, labels = self.actions)\n",
    "            self.loss = tf.reduce_mean(self.discounted_rewards * self.neg_log_proba)\n",
    "            \n",
    "        with tf.name_scope('TrainStep'):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        \n",
    "        with tf.name_scope('SummaryWriter'):\n",
    "            self.train_summary = tf.summary.merge([tf.summary.scalar(\"train_loss\", self.loss)])\n",
    "            self.reward_summary = tf.summary.merge([tf.summary.scalar(\"episode_reward\", self.reward)])\n",
    "            self.mean_reward_summary = tf.summary.merge([tf.summary.scalar(\"mean_episode_reward_50\", self.rewards_mean)])\n",
    "            self.writer = tf.summary.FileWriter(TRAIN_LOG_DIR, session.graph)\n",
    "            \n",
    "        self.init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.session.run(self.init)\n",
    "\n",
    "    def compute_action_proba(self, x):\n",
    "        return self.session.run(self.action_proba, feed_dict={self.states:np.reshape(x,[-1, self.n_in])})\n",
    "    \n",
    "    def train(self, ep_states, ep_actions, ep_rewards):\n",
    "        self.global_step += 1\n",
    "        self.summary, _ = self.session.run([self.train_summary, self.train_step], feed_dict={self.states: ep_states, self.actions: ep_actions, self.discounted_rewards: ep_rewards})\n",
    "        self.writer.add_summary(self.summary, self.global_step)\n",
    "        \n",
    "    def write_reward(self, ep, r, r_mean):\n",
    "        self.summary_0, self.summary_1 = self.session.run([self.reward_summary, self.mean_reward_summary], feed_dict={self.reward: r, self.rewards_mean: r_mean})\n",
    "        self.writer.add_summary(self.summary_0, ep)\n",
    "        self.writer.add_summary(self.summary_1, ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, tf_session, env):\n",
    "        self.env = env\n",
    "        self.obs_space = env.observation_space.shape[0]\n",
    "        try:\n",
    "            self.act_space = env.action_space.n\n",
    "        except:\n",
    "            self.act_space = env.action_space.shape[0]\n",
    "        self.gamma = 0.95\n",
    "        self.policy = Network(tf_session, self.obs_space, self.act_space)\n",
    "        \n",
    "        self.ep_states = []\n",
    "        self.ep_actions = []\n",
    "        self.ep_rewards = []\n",
    "\n",
    "    def gather_exp(self, observation, action, reward):\n",
    "        self.ep_states.append(observation)\n",
    "        self.ep_actions.append(action)\n",
    "        self.ep_rewards.append(reward)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        action_prob = self.policy.compute_action_proba(observation).ravel()\n",
    "        return np.random.choice(list(range(self.act_space)),p = action_prob)\n",
    "    \n",
    "    def choose_random_action(self):\n",
    "        return np.random.choice(list(range(self.act_space)))\n",
    "\n",
    "    def update_policy(self):\n",
    "        self.one_hot_actions = get_one_hot(self.ep_actions, self.act_space)\n",
    "        self.discounted_rewards = get_discounted_values(self.ep_rewards, self.gamma)\n",
    "        self.policy.train(self.ep_states, self.one_hot_actions,self.discounted_rewards)\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def gather_reward(self, reward):\n",
    "        self.ep_rewards.append(reward)\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return sum(self.ep_rewards)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.ep_states = []\n",
    "        self.ep_actions = []\n",
    "        self.ep_rewards = []\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_LOG_DIR):\n",
    "    os.makedirs(MODEL_LOG_DIR)\n",
    "if not os.path.exists(TRAIN_LOG_DIR):\n",
    "    os.makedirs(TRAIN_LOG_DIR)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "sess = tf.Session()\n",
    "player = Player(sess, env)  \n",
    "saver = tf.train.Saver()\n",
    "ep_rewards = []\n",
    "\n",
    "date_object = datetime.now()\n",
    "current_time = date_object.strftime('%H:%M:%S')\n",
    "print(\"Training starts -- {}\".format(current_time))\n",
    "\n",
    "for ep in range(300):\n",
    "    \n",
    "    observation = player.reset()\n",
    "    \n",
    "    while(True):\n",
    "        player.render()\n",
    "        action = player.choose_action(observation)\n",
    "        new_observation, reward, done, _ = player.take_action(action)\n",
    "        player.gather_exp(observation, action, reward)\n",
    "        observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            player.update_policy()\n",
    "            ep_rewards.append(player.get_total_reward())\n",
    "            player.policy.write_reward(ep+1, ep_rewards[-1], np.mean(ep_rewards[-50:]))\n",
    "            print('==========================\\n')\n",
    "            print('Episode {} with reward {}\\n'.format(ep+1, ep_rewards[-1]))\n",
    "            print('Mean reward {}\\n'.format(np.mean(ep_rewards)))\n",
    "            print('Max reward {}\\n'.format(np.max(ep_rewards)))\n",
    "            break\n",
    "\n",
    "saver.save(sess, TRAIN_LOG_DIR, global_step=ep+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
